---
title: "On the MDP formulation of LLMs used in R1"
date: "2025-02-09"
author: "Rao"
---

On the MDP formulation of LLMs used in R1 [Not quite #SundayHarangue]

Everyone knows that R1 is using RL on LLMs. Most also know that RL is done on an underlying MDP formulation. Not everyone might have grokked the rather strange nature of the MDP formulation used. We had a fun 3hr discussion about this in our group meeting last Friday--lead by 
[@soumya_samineni](https://x.com/soumya_samineni)
 & 
[@durgesh_kalwar](https://x.com/durgesh_kalwar)
and I thought I'll summarize for y'all. 

MDP Background: 

An MDP formulation requires Actions (A), States (S), Transition Function (T) and reward function (R). The "solution" to the MDP is a policy π that maps from states to actions.  

T gives the distribution Pr(s'|s,a)--the distribution of states we can expect when a is done in state s. If doing action a in s always leads to a unique state, we have deterministic dynamics and the probability distribution will be degenerate. 

π gives the distribution Pr(a|s)--the distribution of actions the agent will do when it is in state s. If there is a unique action for each state, we have a deterministic policy, otherwise a stochastic policy. 

It is worth noting that the general grid world problems that form the toy examples in RL texts tend to have stochastic dynamics and deterministic policies (thanks to the theorem that if there is an optimal policy for single agent MDP, there will also be a deterministic optimal policy). The rewards are assumed to come from the environment 

(The only reason to consider stochastic policies for single agent MDPs is to make it easier to do policy gradient approaches--since you can tweak the distributions infinitesimally). 

LLM-MDP (as used in R1):

The MDP view here is that the LLM "agent" acts by emitting tokens into the context window (which is effectively the state). The emitted token is just concatenated to the context (thus the transition is deterministic!).  At any given time, the agent (LLM) has a (state-dependent) stochastic policy π completely specified by the LLM parameters, that it uses to emit these tokens.   

The reward function R is specified extensionally. There is a database of training problems with solutions <q_i, sol_i>. Given q_i as the starting prompt, and a trajectory τ generated by the current policy π, the trajectory gets a reward if its suffix contains sol_i (as presumably verified externally).  

(Note that the sequence of solution tokens may have an arbitrary length. R1 distributes this reward over the entire trajectory--with each token getting a (1/trajectory length) fraction of the reward. The reward is best viewed as being just given to the action tokens (i.e., R(s,a,s') = R(a)). 

tldr; 

Actions = individual tokens
States = sequences of tokens
"doing an action" ~ "concatenating action token to state token sequence"  [Thus deterministic transition fn]

The RL optimization process constists of using a policy gradient approach (in R1's case, GRPO) to update the current policy π in the direction of higher return. The objective function has a KL divergence term to incentivize keeping the new policy closer to the old one.

Role of the base LLM

The initial policy comes from the base LLM (which is presumably only trained auto-regressively on the common crawl).  The RL training in R1 expects that the trajectory τ output by base LLM policy effectively has intermediate tokens (that it calls "thoughts") and the solution tokens.  So the base LLM should have been trained on enough derivational trace data that it outputs such trajectories by default. 

The RL policy gradient process can basically be seen adjusting the base LLM parameters so its trajectories are more likely to end up solution-bearing on the training data. 

For more details on how R1 uses the RL post-training see [this tweet](https://x.com/rao2z/status/1886191832071323941)

LLM-MDP vs. Task-MDP

It is worth noting that the LLM-MDP here has no direct semantic connection to the MDP of the actual task that corresponds to any specific test query. If the query is about a grid world problem, the semantic actions there--the go up, go down etc, and the states--cells with coordinates, are both at a different level of abstraction than the LLM-MDP.

While the LLM-MDP has been RL-trained to do well on a certain training population of problems, whether or not that training can help it handle specific tasks during inference (the "generalization claim") is something much harder to predict. 

This is why, compared to Alpha Go/Alpha Zero, the LLM-MDP is best seen as a pseudo-action RL -- the actions LLM-MDP is focusing on are language tokens and not the task actions. (This is in contrast to directly training transformer models on the state-action trajectories of the underlying task--as was done earlier on in projects like decision transformer). 

Appendix:  Alternate LLM-MDP formulations

Part of the point of the above is that there can certainly be other choices for the formulation of LLM-MDP. Interestingly, my original speculation on the LLM-MDP formulation, as I speculated at the time o1 came out, was in terms of two separate LLMs. The base LLM, and a action generator LLM (see [this tweet](https://x.com/rao2z/status/1834354533931385203) & [this tweet](https://x.com/rao2z/status/1865985285034553478)). In this formulation, the base LLM is just a static environment whose parameters don't change, and the move generator LLM focuses on probing the base LLM with prompt augmentations. The RL is used to learn the policy of the move generator LLM.  It will certainly be interest to figure out potential advantages of such alternate formulations.

![LLM-MDP](/images/mdp.jpeg)