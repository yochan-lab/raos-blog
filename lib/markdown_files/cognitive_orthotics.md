---
title: "Why we should view LLMs as powerful Cognitive Orthotics rather than  alternatives for human intelligence"
date: "2023-10-29"
author: "Rao"
---

#SundayHarangue

LLMs are amazing giant external non-veridical memories that can serve as powerful cognitive orthotics for us, if rightly used  (c.f. [this tweet](https://x.com/rao2z/status/1703466122048401563?s=20)).

The trick, IMHO, is to exploit them without deluding ourselves in the process.

The delusion comes chiefly from our incessant need to confuse them for human intelligence,  merrily applying anthropomorphic concepts such as  thinking , thoughts, reasoning and self-critiquing to LLMs. (c.f. [this tweet](https://x.com/rao2z/status/1708329785745928558?s=20);
[this tweet](https://x.com/rao2z/status/1697101605852917861?s=20);
[this tweet](https://x.com/rao2z/status/1648909792390565888?s=20))

This anthropomorphization is quite futile--and, as shown in the case of some of the  current Ersatz Natural Science  AI literature--even counterproductive and misleading.

Sure we didn't quite foresee how impressive the approximate omniscience of these n-gram models on steroids would be, but that doesn't have to make us assume they do everything humans do.

Unless human-level #AI is your singular goal, you don't necessarily need to think auto-regressive LLMs suck (as 
[@ylecun](https://x.com/ylecun) puts it colorfully). 

LLMs can be very effective complementary cognitive orthotics without subsuming human intelligence.

LLMs do some things way way way better than humans do  (the litmus test--from my perspective--being converting anything to iambic pentameter in seconds ðŸ˜‹[this tweet](https://x.com/rao2z/status/1718639333685772692?s=20)),  and do other things (planning, reasoning, self critiquing, mental modeling) much worse (c.f. [this tweet](https://x.com/rao2z/status/1701612863981494522?s=20); [this tweet](https://x.com/rao2z/status/1715800819239678013?s=20); [this tweet](https://x.com/rao2z/status/1643463201462579200?s=20))

If we can manage to tone down the  "LLMs are Zero-shot <XXX>" studies rife with confirmation biases that conflate approximate retrieval for other capabilities associated with human intelligence (c.f. [this tweet](https://x.com/rao2z/status/1553082703251005442?s=20); [this tweet](https://x.com/rao2z/status/1707858010645975112?s=20)), we can:

1.  Focus on the right way of leveraging the strengths of LLMs. This  can certainly be done in LLM-modulo architectures, with either humans or other specialized sound reasoners in the loop).

2. To the extent human-level intelligence is still your holy grail,  keep open research avenues that don't just involve scaling up autoregressive architectures. (It is in this sense that I sympathize with [ylecun](https://x.com/ylecun)'s AR-LLMs suck comment--they do, currently, suck the oxygen out of the research room ðŸ˜…)

tldr; you can get much farther with LLMs & #AI if you think of them as cognitive orthotics and can stop anthropomorphizing them.. (c.f. [this tweet](https://x.com/rao2z/status/1678660725911240704?s=20); 
[this tweet](https://x.com/deliprao/status/1686813112350171136?s=20))