---
title: "Rejecting papers in #AI Conferences because of 'resource constraints' is shooting ourselves in the foot as a community; use Findings.."
date: "2025-11-24"
author: "Rao"
---

#SundayHarangue 

By now, we have all know that top AI conferences are oversubscribed (in terms of paper submissions), and have heard that that they are forced to  lean on the ACs heavily to reduce acceptances--due to "resource constraints."

Let me start by saying that I completely sympathize with the predicament of the organizers of the AI conferences--who have after all being forced to take up increasingly Herculean tasks (see ðŸ‘‡)

https://x.com/rao2z/status/1962187864340648186

I also believe that top AI conferences have generally been a lot more enlightened than other conferences.  Almost all best practices in conference running-- rebuttals, meta reviews, de-stigmatizing poster papers by making all papers posters by default--have come from AI conferences! (cf [this tweet](https://x.com/rao2z/status/1293703721050726400) and [this blog post](https://ijcai-16-pc.blogspot.com/2016/04/))

The best AI conferences have also long stopped bragging about acceptance rates--a metric that still rules the waves in other fields despite its highly dubious motivation (1% junk is still junk; 50% of gold is still gold). 

But lately, there is a new issue that is afflicting the AI conferences--a massive increase in the number of submissions (cf [this tweet](https://x.com/rao2z/status/1960307416119345152)). While it was fun for a while to brag about submission numbers, now almost all big conferences are ruing this trend. 

The reasons for this increase are of course complex--and the 24/7 hype around AI--some of which is more than justified by the impressive advances in the field--is certainly not helping. Almost all engineering students and faculty--no matter what their specific major--are doing things related to AI and justifiably want to join the carnival er.. conference circuit. 

This phenomenon is not exactly going away anytime soon. Efforts to discourage submissions either by haranguing, or weeding out phases--are neither effective nor good for the field!

The most pressing issue of this increase has of course been the burden on the reviewers. While it is clear that there are simply not enough qualified reviewers for looking at the onslaught of papers, we do seem to have found some ways to handle it--e.g. with multi-tiered PCs--Reviewers, ACs, SACs, PC Chairs, and (perhaps less defensible, albeit understandable) conscription policies requiring authors of submitted papers to agree to be reviewers. 

A new, more worrisome trend is that the number of papers with reasonable review scores and accept-worthy AC meta-reviews is so high that the conferences are descending into the unenviable position of not even having enough poster space in the cavernous convention centers they are holding the conferences in.

(As one concrete example, I got an email from a  conference I am AC'ing for saying that a paper that I recommended for acceptance and wrote a meta-review and SAC confidential comments-- falls below the average review rating threshold based on their resource constraints, and that if I want the paper to be rescued, I have to do additional work requesting the SAC for exception..)

While understandable, this practice is basically tantamount to the community shooting ourselves in the foot. It is not like the rejected papers disappear from the face of earth!--we all know that an overwhelming number of them just enter the next conference deadline--that the conferences have nicely coordinated to make happen within a week or so of the decisions of the previous conference! But the reviewers for this other conferences are basically us, and so we are just further exacerbating the reviewer cycle paucity problem.

For the good of the field as well as that of reviewers' time, the goal must be to reduce the need to re-review any papers that have received thoughtful reviews that are generally supportive. 

Here is one idea: 

If physical space for posters has become the crunch point, then perhaps the AI conferences should consider adopting the ACL approach of having main track vs.  findings track.  After all, if we can have rungs above posters--with spotlight and oral categories, then we can also have a rung below posters for papers that have reviewed well but for the poster space crunch!

After all, we already have all papers appearing on arXiv before submission, and these "Findings track" acceptance can serve as a badge of community acceptance--even if it is not of "oral presentation" level. 

I would love to hear other thoughtful ideas.. 

(I reiterate that I am only talking about acceptance caps because of resource constraints. As I mentioned, conferences that try to keep "acceptance rates" below a certain magic number--to curry favors with the tenure committees in various places that would like to continue with their outdated selection rate metrics to bean count research productivity--are too far gone for me to care..)